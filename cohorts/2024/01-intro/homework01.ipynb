{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.12.3\n"
     ]
    }
   ],
   "source": [
    "!python -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LINK_YELLOW_TAXI_DATA_202301 = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet\"\n",
    "LINK_YELLOW_TAXI_DATA_202302 = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-02.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. Downloading the data\n",
    "\n",
    "We'll use [the same NYC taxi dataset](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page),\n",
    "but instead of \"**Green** Taxi Trip Records\", we'll use \"**Yellow** Taxi Trip Records\".\n",
    "\n",
    "Download the data for January and February 2023.\n",
    "\n",
    "Read the data for January. How many columns are there?\n",
    "\n",
    "* 16\n",
    "* 17\n",
    "* 18\n",
    "* **19** (answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet\n",
    "# !wget https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-02.parquet\n",
    "# !wget https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-02.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1= pd.read_parquet(\"yellow_tripdata_2023-01.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['VendorID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count', 'trip_distance', 'RatecodeID', 'store_and_fwd_flag', 'PULocationID', 'DOLocationID', 'payment_type', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'total_amount', 'congestion_surcharge', 'airport_fee']\n",
      "len(list_columns) 19\n"
     ]
    }
   ],
   "source": [
    "list_columns_202301 = list(df1.columns)\n",
    "print(list_columns_202301)\n",
    "print(\"len(list_columns)\", len(list_columns_202301))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. Computing duration\n",
    "\n",
    "Now let's compute the `duration` variable. It should contain the duration of a ride in minutes. \n",
    "\n",
    "What's the standard deviation of the trips duration in January?\n",
    "\n",
    "* 32.59\n",
    "* **42.59** (answer)\n",
    "* 52.59\n",
    "* 62.59"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df1['duration'].std() 42.594351241920904\n"
     ]
    }
   ],
   "source": [
    "df1['duration_td'] = df1['tpep_dropoff_datetime'] - df1['tpep_pickup_datetime']\n",
    "df1['duration'] = df1['duration_td'].apply(lambda td: td.total_seconds() / 60)\n",
    "std_dev = df1['duration'].std()\n",
    "print(\"df1['duration'].std()\", std_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. Dropping outliers\n",
    "\n",
    "Next, we need to check the distribution of the `duration` variable. There are some outliers. Let's remove them and keep only the records where the duration was between 1 and 60 minutes (inclusive).\n",
    "\n",
    "What fraction of the records left after you dropped the outliers?\n",
    "\n",
    "* 90%\n",
    "* 92%\n",
    "* 95%\n",
    "* **98%** (answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio: 0.9812202822125979\n"
     ]
    }
   ],
   "source": [
    "original_len = len(df1)\n",
    "\n",
    "df1 = df1[(df1['duration'] >= 1) & (df1['duration'] <= 60)]\n",
    "\n",
    "new_len = len(df1)\n",
    "ratio = new_len / original_len\n",
    "\n",
    "print(\"ratio:\", ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import root_mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4. One-hot encoding\n",
    "\n",
    "Let's apply one-hot encoding to the pickup and dropoff location IDs. We'll use only these two features for our model. \n",
    "\n",
    "* Turn the dataframe into a list of dictionaries (remember to re-cast the ids to strings - otherwise it will \n",
    "  label encode them)\n",
    "* Fit a dictionary vectorizer \n",
    "* Get a feature matrix from it\n",
    "\n",
    "What's the dimensionality of this matrix (number of columns)?\n",
    "\n",
    "* 2\n",
    "* 155\n",
    "* 345\n",
    "* **515** (answer)\n",
    "* 715"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1['PU_DO'] = df1['PULocationID'].astype(str) + '_' + df1['DOLocationID'].astype(str)\n",
    "df1['PU'] = 'PU_' + df1['PULocationID'].astype(str)\n",
    "df1['DO'] = 'DO_' + df1['DOLocationID'].astype(str)\n",
    "\n",
    "\n",
    "categorical = ['PU', 'DO']\n",
    "# categorical = ['PU_DO']\n",
    "numerical = []\n",
    "\n",
    "dv = DictVectorizer()\n",
    "\n",
    "train_dicts = df1[categorical + numerical].to_dict(orient='records')\n",
    "X_train = dv.fit_transform(train_dicts)\n",
    "\n",
    "target = 'duration'\n",
    "y_train = df1[target].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3009173, 515)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5. Training a model\n",
    "\n",
    "Now let's use the feature matrix from the previous step to train a model. \n",
    "\n",
    "* Train a plain linear regression model with default parameters \n",
    "* Calculate the RMSE of the model on the training data\n",
    "\n",
    "What's the RMSE on train?\n",
    "\n",
    "* 3.64\n",
    "* **7.64** (answer)\n",
    "* 11.64\n",
    "* 16.64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.649261929201487"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lr.predict(X_train)\n",
    "\n",
    "# mean_squared_error(y_train, y_pred, squared=False)\n",
    "root_mean_squared_error(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6. Evaluating the model\n",
    "\n",
    "Now let's apply this model to the validation dataset (February 2022). \n",
    "\n",
    "What's the RMSE on validation?\n",
    "\n",
    "* 3.81\n",
    "* **7.81** (answer)\n",
    "* 11.81\n",
    "* 16.81"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_parquet(\"yellow_tripdata_2023-02.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "PU_set = set(df1['PU'])\n",
    "DO_set = set(df1['DO'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['duration_td'] = df2['tpep_dropoff_datetime'] - df2['tpep_pickup_datetime']\n",
    "df2['duration'] = df2['duration_td'].apply(lambda td: td.total_seconds() / 60)\n",
    "\n",
    "df2 = df2[(df2['duration'] >= 1) & (df2['duration'] <= 60)]\n",
    "\n",
    "df2['PU'] = 'PU_' + df2['PULocationID'].astype(str)\n",
    "df2['DO'] = 'DO_' + df2['DOLocationID'].astype(str)\n",
    "\n",
    "\n",
    "# df2 = df2[df2['PU'].isin(PU_set) & df2['DO'].isin(DO_set)]\n",
    "\n",
    "categorical = ['PU', 'DO']\n",
    "# categorical = ['PU_DO']\n",
    "numerical = []\n",
    "\n",
    "val_dicts = df2[categorical + numerical].to_dict(orient='records')\n",
    "X_val = dv.transform(val_dicts)\n",
    "\n",
    "target = 'duration'\n",
    "y_val = df2[target].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.811819793542861"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lr2 = LinearRegression()\n",
    "# lr2.fit(X_train, y_train)\n",
    "y_pred_val = lr.predict(X_val)\n",
    "\n",
    "# mean_squared_error(y_val, y_pred, squared=False)\n",
    "root_mean_squared_error(y_val, y_pred_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops_hw1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
